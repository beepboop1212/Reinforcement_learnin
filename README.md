# ðŸ¤– Q-Learning Maze Solver

This project is an implementation of a **Q-Learning agent** that learns to navigate a maze from a start position to a goal using **reinforcement learning**. The agent learns through trial and error, receiving rewards for reaching the goal and penalties for hitting walls or taking unnecessary steps.

---

## ðŸ§  Key Concepts

- **Q-Learning**: A value-based Reinforcement Learning algorithm used to find the optimal action-selection policy.
- **Exploration vs Exploitation**: The agent balances between exploring the environment and exploiting learned knowledge.
- **Reward System**:
  - âœ… Goal: +100
  - âŒ Wall: -10
  - âž– Step: -1

---

## ðŸ§° Technologies Used

- Python
- NumPy
- Matplotlib

---

## ðŸ§± Maze Structure

The maze is a 2D NumPy array where:
- `0` = Free space
- `1` = Wall

Example:

```python
maze_layout = np.array([
    [0, 1, 1, 0, 0],
    [0, 0, 1, 1, 0],
    [0, 0, 0, 0, 0],
    [1, 1, 0, 1, 1],
    [0, 0, 0, 0, 0]
])



â¸»

ðŸ§ª How It Works
	1.	Initialize Maze with start and goal positions.
	2.	Q-Learning Agent begins with no knowledge (Q-values initialized to zero).
	3.	Agent explores the maze across multiple episodes, updating its Q-table based on:
	â€¢	Immediate reward
	â€¢	Future expected rewards
	4.	Once trained, the agent can find optimal or near-optimal paths to the goal.

â¸»

ðŸ“ˆ Training Results

Example training (100 episodes):
	â€¢	âœ… Average Reward: ~29
	â€¢	ðŸ“‰ Average Steps: ~20

After training:
	â€¢	Optimal paths found within 8â€“10 steps
	â€¢	Reward increased to ~90+

â¸»

ðŸš€ How to Run

1. Install Dependencies

pip install numpy matplotlib

2. Run the Script

Simply run the Python script or Jupyter Notebook.

python maze_solver.py

3. Output
	â€¢	The agentâ€™s path is printed and visualized
	â€¢	Path shows start (S), goal (G), and learned path (#)
	â€¢	Episode stats and Q-learning progress are plotted

â¸»

ðŸ” Sample Output

Learned Path:
(0, 0)-> (0, 1)-> (1, 1)-> (1, 2)-> (2, 2)-> (2, 3)-> (2, 4)-> (3, 4)-> Goal!
Number of steps: 8
Total reward: 93



â¸»

ðŸ“Œ Notes
	â€¢	This project simulates how AI agents learn environments without supervision.
	â€¢	You can modify maze size, start/goal, or rewards to experiment with different dynamics.
	â€¢	Agent uses 4 actions: Up, Down, Left, Right.

â¸»


ðŸ§  Authorâ€™s Tip

Start with a small maze and increase complexity as the agent learns. Try changing penalties or discount factor to observe agent behavior!

---
